{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linnaeus University\n",
    "## Introduction to Machine learning, 25VT-2DV516\n",
    "## Assignment 1\n",
    "\n",
    "**Name:** ## Your name here ## \n",
    "\n",
    "**Email:** ## Your email here ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this assignment you will handle four exercises related to the k-Nearest Neighbors algorithm.\n",
    "The main purpose is to get you up and running using Python, NumPy and Matplotlib. \n",
    "The library Scipy will be used specifically in Exercise 3, part 2.\n",
    "\n",
    "## Submission Instructions\n",
    "\n",
    "All exercises are individual. We expect you to submit a zip file with this notebook with your solutions and the MachineLearning.py with the models implemented. \n",
    "You must normalize your data before doing anything with your data.\n",
    "When grading your assignments we will in addition to functionality also take into account code quality. \n",
    "We expect well structured and efficient solutions. \n",
    "Finally, keep all your files in a single folder named as username_A1 and submit a zipped version of this folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Models implementation and testing (All Mandatory)\n",
    "\n",
    "1. Implement all the methods in the abstract classes **KNNRegressionModel** and **KNNClassificationModel** in the MachineLearningModel.py file. \n",
    "As the names suggest, you must implement the Regression (slide 30) and Classification (slide 24) versions of the KNN algorithm and you must follow the algorithms stated in the slides. \n",
    "* Both models must use the Euclidean distance as the distance function (*Tip: Code smart by implementing an auxiliary method _euclidian_distance() in the MachineLearningModel.py file*).\n",
    "* The evaluate() function for the **KNNRegressionModel** must implement the Mean Squared Error (MSE)\n",
    "* The evaluate() function for the **KNNClassificationModel** must count the number of correct predictions.\n",
    "\n",
    "2. Use the *Polynomial200.csv* dataset to show that all your methods for the **KNNRegressionModel** is working as expected. You must produce a similar figure to the one in slide 31. Instructions to produce the figure are present in the slide. You must show the effects of using k = 3, 5, 7 and 9 and discuss your findings on the figure produced.\n",
    "\n",
    "**Discuss your findings for this question below**\n",
    "\n",
    "----- Your answer here -----\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Your code here ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "3. Use the *IrisDataset.csv* dataset to show that all your methods for the **KNNClassificationModel** is working as expected. You must produce a similar figure to the one in slide 28. Instructions on how to produce the figure are given in the slide. You must choose 2 input variables only to produce the figure (they do not need to match the figure in the slide). You must show the effects of using k = 3, 5, 7, and 9 and discuss the figure produced.\n",
    "\n",
    "**Tips**\n",
    "\n",
    "* Check the function *np.meshgrid* from numpy to create the samples.\n",
    "* Check the function *plt.contourf* for generating the countours. \n",
    "* There are many tutorials online to produce this figure. Find one that most suits you.\n",
    "\n",
    "**Discuss your findings for this question below**\n",
    "\n",
    "----- Your answer here -----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Your code here ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: KNN Regression (Mandatory)\n",
    "\n",
    "1. (Mandatory) Create a procedure to repeat 10 times the following strategy.\n",
    "* Use the values for k = 3, 5, 7, 9, 11, 13 and 15.\n",
    "* Split your dataset randomly into 80% for training, and 20% testing. Use 10 different seeds for splitting the data.\n",
    "* Evaluate (MSE implemented in your class) your **KNNRegressionModel** for each k in the **test set** and store the result. \n",
    "* Plot a barchart with these results.\n",
    "\n",
    "Which k gives the best regression? Motivate your answer!\n",
    "\n",
    "**Discuss your findings for this question below**\n",
    "\n",
    "----- Your answer here -----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Your code here ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: KNN Classification (1 Mandatory , 1 Non-Mandatory)\n",
    "\n",
    "1. **(Mandatory)** Using the **IrisDataset.csv**, find the best combination of two features that produces the best model using **KNNClassificationModel**.\n",
    "* You must try all combinations of two features, and for k = 3, 5, 7, and 9.\n",
    "* You must use plots to support your answer.\n",
    "\n",
    "**Discuss your findings for this question below**\n",
    "\n",
    "----- Your answer here -----\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Your code here ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **(Non-mandatory)** Implement a new Class called **FastKNNClassificationModel**. This method should be faster than your regular implementation. This can be done by using a faster data structure to look for the closest neighbors faster. In this assignment, you must build the KDTree with the the training data and then search for the neighbors using it.\n",
    "\n",
    "* You must use this implementation of KDTree from Scipy. https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.KDTree.html\n",
    "* The methods needed for your implementation are only the *constructor* (to build the KDTree) and the method *query* to find the k-neighbors.\n",
    "* You must design an experiment using the **IrisDataset.csv** with **all features** to show that your new implementation is faster than your implementation of **KNNClassificationModel**.\n",
    "* For example, you can measure the time using of each prediction, for each classifier, and plot the average time to give a decision for entries. Also, measure how this would increase/decrease with the increment of the input parameter *k*. \n",
    "* Use a plot(s) from matplotlib to support your answer.\n",
    "\n",
    "**Discuss your findings for this question below**\n",
    "\n",
    "----- Your answer here -----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Your code here ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: MNIST k-NN classification (Non-mandatory)\n",
    "\n",
    "In this final exercise, we will use k-NN for classifying handwritten digits using the very famous MNIST dataset. Input to the algorithm is an image (28x28 pixel) with a handwritten digit (0-9) and the output should be a classification 0-9. The dataset and a description of it is available at http://yann.lecun.com/exdb/mnist/. Google MNIST Python to learn how to access it. The objective is to use your k-NN classifier to perform as good as possible on recognizing handwritten images. Describe your effort and what you found out to be the best k to lower the test error. The complete dataset has 60,000 digits for training and 10,000 digits for testing. Hence the computations might be heavy, so start of by a smaller subset rather than using the entire dataset. The final testing should (if possible) be done for the full test set but we will accept solutions that use \"only\" 10,000 digits for training and 1,000 digits for testing.\n",
    "The description of this exercise is deliberately vague as you are supposed to, on your own, find a suitable way to solve this problem in detail. This is why it is important that you document your effort and progress in your report. **You must use your implementations of KNN for classification. If you successfully finished Exercise 3, part 2, it is advisable to use your FastKNNClassificationModel**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Your code here ###"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
